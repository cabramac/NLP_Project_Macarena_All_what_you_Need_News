{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a6bbe54",
   "metadata": {},
   "source": [
    "## **5. Predict on Validation Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfedcdae",
   "metadata": {},
   "source": [
    "5.1 Importing the corresponding libraries and defining the path directory of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682c7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, joblib, pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "TFIDF_PATH = \"tfidf_vectorizer.pkl\"\n",
    "MODEL_PATH = \"svm_tfidf.pkl\"\n",
    "\n",
    "VAL_READY  = \"validation_ready.csv\" \n",
    "VAL_RAW    = \"validation_data.csv\"  \n",
    "OUT_PATH   = \"validation_data_predicted.csv\"\n",
    "\n",
    "CAND_TEXT_COLS: List[str] = [\"text\", \"content\", \"article\", \"body\", \"full_text\", \"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc6572",
   "metadata": {},
   "source": [
    "5.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621200c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved predictions -> validation_data_predicted.csv\n",
      "Label counts: {0: 3465, 1: 1491}\n"
     ]
    }
   ],
   "source": [
    "# ---------- helpers ----------\n",
    "def sniff_delimiter(path: str, default: str = \",\") -> str:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sample = f.read(2048)\n",
    "        return csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def find_text_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def ensure_text_clean(df_in: pd.DataFrame, text_col: Optional[str]) -> Tuple[pd.DataFrame, str]:\n",
    "    df = df_in.copy()\n",
    "    if \"text_clean\" in df.columns:\n",
    "        return df, \"text_clean\"\n",
    "\n",
    "    chosen = text_col or find_text_column(df, CAND_TEXT_COLS)\n",
    "    if chosen is None:\n",
    "        raise ValueError(\n",
    "            f\"Couldn't find a text column. Looked for {CAND_TEXT_COLS}. \"\n",
    "            f\"Columns in your file: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    \n",
    "    if \"text_preprocessing\" in globals():\n",
    "        df = text_preprocessing(df, text_col=chosen)\n",
    "        if \"text_clean\" not in df.columns:\n",
    "            raise KeyError(\"text_preprocessing() did not create 'text_clean'.\")\n",
    "        return df, \"text_clean\"\n",
    "\n",
    "    # Minimal fallback cleaner (only if your pipeline isn't in memory)\n",
    "    import re, string\n",
    "    def _basic_clean_txt(s):\n",
    "        s = \"\" if s is None else str(s)\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
    "        s = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    df[\"text_clean\"] = df[chosen].map(_basic_clean_txt)\n",
    "    return df, \"text_clean\"\n",
    "\n",
    "def load_artifacts(tfidf_path: str, model_path: str):\n",
    "    tfidf = joblib.load(tfidf_path)\n",
    "    model = joblib.load(model_path)\n",
    "    return tfidf, model\n",
    "\n",
    "def predict_labels(df_proc: pd.DataFrame, tfidf, model, clean_col: str = \"text_clean\") -> pd.Series:\n",
    "    X = tfidf.transform(df_proc[clean_col].fillna(\"\"))\n",
    "    preds = model.predict(X)\n",
    "    return pd.Series(preds, index=df_proc.index, name=\"label_pred\")\n",
    "\n",
    "def apply_predictions_replace_twos(df_orig: pd.DataFrame, preds: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a copy where ONLY rows with label==2 are replaced with predictions (0/1).\n",
    "    If 'label' doesn't exist, it will be created from preds.\n",
    "    \"\"\"\n",
    "    df_out = df_orig.copy()\n",
    "    if \"label\" not in df_out.columns:\n",
    "        df_out[\"label\"] = preds.values\n",
    "        return df_out\n",
    "    mask = (df_out[\"label\"] == 2)\n",
    "    df_out.loc[mask, \"label\"] = preds.loc[mask].values\n",
    "    return df_out\n",
    "\n",
    "def write_like_input(df_out: pd.DataFrame, df_input: pd.DataFrame, input_path: str, out_path: str):\n",
    "    \n",
    "    original_cols = list(df_input.columns)\n",
    "    cols_to_write = original_cols[:] if \"label\" in original_cols else original_cols + [\"label\"]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True) if os.path.dirname(out_path) else None\n",
    "    delim = sniff_delimiter(input_path, default=\",\")\n",
    "\n",
    "    df_out[cols_to_write].to_csv(\n",
    "        out_path,\n",
    "        sep=delim,\n",
    "        index=False,\n",
    "        lineterminator=\"\\n\",\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    return out_path\n",
    "\n",
    "# ---------- main ----------\n",
    "def predict_and_replace_on_validation():\n",
    "    tfidf, model = load_artifacts(TFIDF_PATH, MODEL_PATH)\n",
    "\n",
    "    # Prefer the ready file if it exists; otherwise use the raw one\n",
    "    if os.path.exists(VAL_READY):\n",
    "        src_path = VAL_READY\n",
    "    elif os.path.exists(VAL_RAW):\n",
    "        src_path = VAL_RAW\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Neither {VAL_READY} nor {VAL_RAW} was found.\")\n",
    "\n",
    "    df_in = pd.read_csv(src_path, sep=None, engine=\"python\")\n",
    "\n",
    "    # Build a 'text_clean' column (or reuse it)\n",
    "    df_proc, clean_col = ensure_text_clean(df_in, text_col=None)\n",
    "\n",
    "    # Predict\n",
    "    preds = predict_labels(df_proc, tfidf, model, clean_col=clean_col)\n",
    "\n",
    "    # Replace only the 2s (or create label if missing)\n",
    "    df_out = apply_predictions_replace_twos(df_in, preds)\n",
    "\n",
    "    # Save preserving original format\n",
    "    out_path = OUT_PATH if OUT_PATH else src_path\n",
    "    out_path = write_like_input(df_out, df_in, src_path, out_path)\n",
    "\n",
    "    # Small sanity report\n",
    "    print(f\"[OK] Saved predictions -> {out_path}\")\n",
    "    if \"label\" in df_out.columns:\n",
    "        print(\"Label counts:\", df_out[\"label\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "# run it\n",
    "predict_and_replace_on_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22dba63",
   "metadata": {},
   "source": [
    "Key (0) and (1) are the two classes that my model predicts. This means 0= Fake and 1= Real news. \n",
    "- 3,442 rows were predicted as class 0\n",
    "- 1,491 rows were predicted as class 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IronHack1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

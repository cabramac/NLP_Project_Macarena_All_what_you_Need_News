{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b990b0d",
   "metadata": {},
   "source": [
    "## **NLP Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936e940",
   "metadata": {},
   "source": [
    "Status Quo:\n",
    "2 Datases provided\n",
    "1) Data.csv --> Used for training and testing. Where labels of (0) represent fake news, and (1) real news\n",
    "2) Validation_data.csv --> Data without any real answers. It contains the label filled with 2 as placeholder. \n",
    "In each dataset, we have:\n",
    "Rows → 1 news article \n",
    "Columns → 5 columns with a piece of information ( label, title, text, subject, and date) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa39f7",
   "metadata": {},
   "source": [
    "## **1. Data Understanding & Set-up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2a98b",
   "metadata": {},
   "source": [
    "1.1 Importing libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28d4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de3b9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\macat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\macat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\macat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\macat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\macat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\macat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\macat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a086b2",
   "metadata": {},
   "source": [
    "1.2 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc53676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (39942, 5) | Validation shape: (4956, 5)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data.csv\")      \n",
    "val   = pd.read_csv(\"validation_data.csv\")\n",
    "print(\"Train shape:\", train.shape, \"| Validation shape:\", val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a860f4",
   "metadata": {},
   "source": [
    "39,942 rows → each row is one news article in your training dataset.\n",
    "5 columns → the features: label, title, text, subject, date.\n",
    "\n",
    "4,956 rows → each row is one news article in the validation set (unseen by the model during training).\n",
    "5 columns → same structure as the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511750b5",
   "metadata": {},
   "source": [
    "1.3 Verification of schema. Checking if there are no typos, no missing values in the columns.Confirm the 5 expected columns exist and see their types; check for nulls. This prevents downstream errors and shows data quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ba17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset schema & missing values:\n",
      "          dtype  missing_count  missing_ratio\n",
      "label     int64              0            0.0\n",
      "title    object              0            0.0\n",
      "text     object              0            0.0\n",
      "subject  object              0            0.0\n",
      "date     object              0            0.0\n"
     ]
    }
   ],
   "source": [
    "info_df = pd.DataFrame({\n",
    "    \"dtype\": train.dtypes,\n",
    "    \"missing_count\": train.isna().sum(),\n",
    "    \"missing_ratio\": (train.isna().sum() / len(train)).round(4)\n",
    "})\n",
    "\n",
    "print(\"\\nDataset schema & missing values:\")\n",
    "print(info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733c000",
   "metadata": {},
   "source": [
    "1.4 Checked duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff43f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 3513\n",
      "       count   ratio\n",
      "label               \n",
      "0      19943  0.4993\n",
      "1      19999  0.5007\n"
     ]
    }
   ],
   "source": [
    "dupes = train.duplicated(subset=[\"title\", \"text\"]).sum()\n",
    "print(\"Duplicates:\", dupes)\n",
    "\n",
    "vc = train[\"label\"].value_counts().sort_index()\n",
    "print(pd.DataFrame({\"count\": vc, \"ratio\": (vc/len(train)).round(4)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a4db7",
   "metadata": {},
   "source": [
    " 39,942 total articles in your training data, 3,513 rows have the exact same title and text as another row. (8% of my dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c5880",
   "metadata": {},
   "source": [
    "## **2. Sentiment Analyisis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31bc37",
   "metadata": {},
   "source": [
    "2.1 Cleaning and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d270f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5084b",
   "metadata": {},
   "source": [
    "2.2 Tokenization / Stop word Removal / Steamming / Lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f73b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP = set(ENGLISH_STOP_WORDS)\n",
    "STEMMER = PorterStemmer()\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Lowercase, strip URLs/HTML, regex-tokenize to letters/apostrophes, drop very short tokens.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)  \n",
    "    text = re.sub(r'<.*?>', ' ', text)          \n",
    "    tokens = re.findall(r\"[a-z']+\", text)           # regex tokenization (removal punctuation)\n",
    "    return [t for t in tokens if len(t) > 2]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b7d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in STOP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0c6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    return [LEMMATIZER.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bcdbe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', 'reuters', 'head', 'conservative', 'republican', 'faction', 'congress', 'voted', 'month', 'huge', 'expansion', 'national', 'debt', 'pay', 'tax', 'cut', 'called', 'fiscal', 'conservative', 'sunday', 'urged', 'budget', 'restraint', 'keeping', 'sharp', 'pivot', 'way', 'republican', 'representative', 'mark', 'meadow', 'speaking', 'cbs', 'face', 'nation', 'drew', 'hard', 'line', 'federal', 'spending', 'lawmaker', 'bracing', 'battle', 'january', 'return', 'holiday', 'wednesday', 'lawmaker', 'begin', 'trying', 'pas', 'federal', 'budget', 'fight', 'likely', 'linked', 'issue', 'immigration', 'policy', 'november', 'congressional', 'election', 'campaign', 'approach', 'republican', 'seek', 'control', 'congress', 'president', 'donald', 'trump', 'republican', 'want', 'big', 'budget', 'increase', 'military', 'spending', 'democrat', 'want', 'proportional', 'increase', 'non', 'defense', 'discretionary', 'spending', 'program', 'support', 'education', 'scientific', 'research', 'infrastructure', 'public', 'health', 'environmental', 'protection', 'trump', 'administration', 'willing', 'say', 'going', 'increase', 'non', 'defense', 'discretionary', 'spending', 'percent', 'meadow', 'chairman', 'small', 'influential', 'house', 'freedom', 'caucus', 'said', 'program', 'democrat', 'saying', 'need', 'government', 'pay', 'raise', 'percent', 'fiscal', 'conservative', 'don', 'rationale', 'eventually', 'run', 'people', 'money', 'said', 'meadow', 'republican', 'voted', 'late', 'december', 'party', 'debt', 'financed', 'tax', 'overhaul', 'expected', 'balloon', 'federal', 'budget', 'deficit', 'add', 'trillion', 'year', 'trillion', 'national', 'debt', 'interesting', 'hear', 'mark', 'talk', 'fiscal', 'responsibility', 'democratic', 'representative', 'joseph', 'crowley', 'said', 'cbs', 'crowley', 'said', 'republican', 'tax', 'require', 'united', 'state', 'borrow', 'trillion', 'paid', 'future', 'generation', 'finance', 'tax', 'cut', 'corporation', 'rich', 'fiscally', 'responsible', 'bill', 'seen', 'passed', 'history', 'house', 'representative', 'think', 'going', 'paying', 'year', 'come', 'crowley', 'said', 'republican', 'insist', 'tax', 'package', 'biggest', 'tax', 'overhaul', 'year', 'boost', 'economy', 'job', 'growth', 'house', 'speaker', 'paul', 'ryan', 'supported', 'tax', 'recently', 'went', 'meadow', 'making', 'clear', 'radio', 'interview', 'welfare', 'entitlement', 'reform', 'party', 'call', 'republican', 'priority', 'republican', 'parlance', 'entitlement', 'program', 'mean', 'food', 'stamp', 'housing', 'assistance', 'medicare', 'medicaid', 'health', 'insurance', 'elderly', 'poor', 'disabled', 'program', 'created', 'washington', 'assist', 'needy', 'democrat', 'seized', 'ryan', 'early', 'december', 'remark', 'saying', 'showed', 'republican', 'try', 'pay', 'tax', 'overhaul', 'seeking', 'spending', 'cut', 'social', 'program', 'goal', 'house', 'republican', 'seat', 'senate', 'vote', 'democrat', 'needed', 'approve', 'budget', 'prevent', 'government', 'shutdown', 'democrat', 'use', 'leverage', 'senate', 'republican', 'narrowly', 'control', 'defend', 'discretionary', 'non', 'defense', 'program', 'social', 'spending', 'tackling', 'issue', 'dreamer', 'people', 'brought', 'illegally', 'country', 'child', 'trump', 'september', 'march', 'expiration', 'date', 'deferred', 'action', 'childhood', 'arrival', 'daca', 'program', 'protects', 'young', 'immigrant', 'deportation', 'provides', 'work', 'permit', 'president', 'said', 'recent', 'twitter', 'message', 'want', 'funding', 'proposed', 'mexican', 'border', 'wall', 'immigration', 'law', 'change', 'exchange', 'agreeing', 'help', 'dreamer', 'representative', 'debbie', 'dingell', 'told', 'cbs', 'did', 'favor', 'linking', 'issue', 'policy', 'objective', 'wall', 'funding', 'need', 'daca', 'clean', 'said', 'wednesday', 'trump', 'aide', 'meet', 'congressional', 'leader', 'discus', 'issue', 'followed', 'weekend', 'strategy', 'session', 'trump', 'republican', 'leader', 'jan', 'white', 'house', 'said', 'trump', 'scheduled', 'meet', 'sunday', 'florida', 'republican', 'governor', 'rick', 'scott', 'want', 'emergency', 'aid', 'house', 'passed', 'billion', 'aid', 'package', 'hurricane', 'florida', 'texas', 'puerto', 'rico', 'wildfire', 'california', 'package', 'far', 'exceeded', 'billion', 'requested', 'trump', 'administration', 'senate', 'voted', 'aid']\n"
     ]
    }
   ],
   "source": [
    "sample = str(train.iloc[0][\"text\"])\n",
    "tokens_lemma = lemmatize_tokens(remove_stopwords(tokenize_text(sample)))\n",
    "print(tokens_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f75b2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    return [STEMMER.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c853cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', 'reuter', 'head', 'conserv', 'republican', 'faction', 'congress', 'vote', 'month', 'huge', 'expans', 'nation', 'debt', 'pay', 'tax', 'cut', 'call', 'fiscal', 'conserv', 'sunday', 'urg', 'budget', 'restraint', 'keep', 'sharp', 'pivot', 'way', 'republican', 'repres', 'mark', 'meadow', 'speak', 'cb', 'face', 'nation', 'drew', 'hard', 'line', 'feder', 'spend', 'lawmak', 'brace', 'battl', 'januari', 'return', 'holiday', 'wednesday', 'lawmak', 'begin', 'tri', 'pass', 'feder', 'budget', 'fight', 'like', 'link', 'issu', 'immigr', 'polici', 'novemb', 'congression', 'elect', 'campaign', 'approach', 'republican', 'seek', 'control', 'congress', 'presid', 'donald', 'trump', 'republican', 'want', 'big', 'budget', 'increas', 'militari', 'spend', 'democrat', 'want', 'proport', 'increas', 'non', 'defens', 'discretionari', 'spend', 'program', 'support', 'educ', 'scientif', 'research', 'infrastructur', 'public', 'health', 'environment', 'protect', 'trump', 'administr', 'will', 'say', 'go', 'increas', 'non', 'defens', 'discretionari', 'spend', 'percent', 'meadow', 'chairman', 'small', 'influenti', 'hous', 'freedom', 'caucu', 'said', 'program', 'democrat', 'say', 'need', 'govern', 'pay', 'rais', 'percent', 'fiscal', 'conserv', 'don', 'rational', 'eventu', 'run', 'peopl', 'money', 'said', 'meadow', 'republican', 'vote', 'late', 'decemb', 'parti', 'debt', 'financ', 'tax', 'overhaul', 'expect', 'balloon', 'feder', 'budget', 'deficit', 'add', 'trillion', 'year', 'trillion', 'nation', 'debt', 'interest', 'hear', 'mark', 'talk', 'fiscal', 'respons', 'democrat', 'repres', 'joseph', 'crowley', 'said', 'cb', 'crowley', 'said', 'republican', 'tax', 'requir', 'unit', 'state', 'borrow', 'trillion', 'paid', 'futur', 'gener', 'financ', 'tax', 'cut', 'corpor', 'rich', 'fiscal', 'respons', 'bill', 'seen', 'pass', 'histori', 'hous', 'repres', 'think', 'go', 'pay', 'year', 'come', 'crowley', 'said', 'republican', 'insist', 'tax', 'packag', 'biggest', 'tax', 'overhaul', 'year', 'boost', 'economi', 'job', 'growth', 'hous', 'speaker', 'paul', 'ryan', 'support', 'tax', 'recent', 'went', 'meadow', 'make', 'clear', 'radio', 'interview', 'welfar', 'entitl', 'reform', 'parti', 'call', 'republican', 'prioriti', 'republican', 'parlanc', 'entitl', 'program', 'mean', 'food', 'stamp', 'hous', 'assist', 'medicar', 'medicaid', 'health', 'insur', 'elderli', 'poor', 'disabl', 'program', 'creat', 'washington', 'assist', 'needi', 'democrat', 'seiz', 'ryan', 'earli', 'decemb', 'remark', 'say', 'show', 'republican', 'tri', 'pay', 'tax', 'overhaul', 'seek', 'spend', 'cut', 'social', 'program', 'goal', 'hous', 'republican', 'seat', 'senat', 'vote', 'democrat', 'need', 'approv', 'budget', 'prevent', 'govern', 'shutdown', 'democrat', 'use', 'leverag', 'senat', 'republican', 'narrowli', 'control', 'defend', 'discretionari', 'non', 'defens', 'program', 'social', 'spend', 'tackl', 'issu', 'dreamer', 'peopl', 'brought', 'illeg', 'countri', 'children', 'trump', 'septemb', 'march', 'expir', 'date', 'defer', 'action', 'childhood', 'arriv', 'daca', 'program', 'protect', 'young', 'immigr', 'deport', 'provid', 'work', 'permit', 'presid', 'said', 'recent', 'twitter', 'messag', 'want', 'fund', 'propos', 'mexican', 'border', 'wall', 'immigr', 'law', 'chang', 'exchang', 'agre', 'help', 'dreamer', 'repres', 'debbi', 'dingel', 'told', 'cb', 'did', 'favor', 'link', 'issu', 'polici', 'object', 'wall', 'fund', 'need', 'daca', 'clean', 'said', 'wednesday', 'trump', 'aid', 'meet', 'congression', 'leader', 'discuss', 'issu', 'follow', 'weekend', 'strategi', 'session', 'trump', 'republican', 'leader', 'jan', 'white', 'hous', 'said', 'trump', 'schedul', 'meet', 'sunday', 'florida', 'republican', 'governor', 'rick', 'scott', 'want', 'emerg', 'aid', 'hous', 'pass', 'billion', 'aid', 'packag', 'hurrican', 'florida', 'texa', 'puerto', 'rico', 'wildfir', 'california', 'packag', 'far', 'exceed', 'billion', 'request', 'trump', 'administr', 'senat', 'vote', 'aid']\n"
     ]
    }
   ],
   "source": [
    "sample = str(train.iloc[0][\"text\"])\n",
    "tokens_filtered = stem_tokens(remove_stopwords(tokenize_text(sample)))\n",
    "print(tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ab47c",
   "metadata": {},
   "source": [
    "2.3 Visualizing the words after applying the different techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eef65208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "--- VISUAL TOKEN TRANSFORMATIONS ---\u001b[0m\n",
      "\u001b[34mOriginal tokens:\u001b[0m ['washington', 'reuters', 'the', 'head', 'conservative', 'republican', 'faction', 'the', 'congress', 'who', 'voted', 'this', 'month', 'for', 'huge', 'expansion', 'the', 'national', 'debt', 'pay', 'for', 'tax', 'cuts', 'called', 'himself']\n",
      "\u001b[32mAfter stopword removal:\u001b[0m ['washington', 'reuters', 'head', 'conservative', 'republican', 'faction', 'congress', 'voted', 'month', 'huge', 'expansion', 'national', 'debt', 'pay', 'tax', 'cuts', 'called', 'fiscal', 'conservative', 'sunday', 'urged', 'budget', 'restraint', 'keeping', 'sharp']\n",
      "\u001b[33mAfter stemming (changes in red):\u001b[0m ['washington', '\\x1b[31mreuter\\x1b[0m', 'head', '\\x1b[31mconserv\\x1b[0m', 'republican', 'faction', 'congress', '\\x1b[31mvote\\x1b[0m', 'month', 'huge', '\\x1b[31mexpans\\x1b[0m', '\\x1b[31mnation\\x1b[0m', 'debt', 'pay', 'tax', '\\x1b[31mcut\\x1b[0m', '\\x1b[31mcall\\x1b[0m', 'fiscal', '\\x1b[31mconserv\\x1b[0m', 'sunday', '\\x1b[31murg\\x1b[0m', 'budget', 'restraint', '\\x1b[31mkeep\\x1b[0m', 'sharp']\n",
      "\u001b[33mAfter lemmatization (changes in red):\u001b[0m ['washington', 'reuters', 'head', 'conservative', 'republican', 'faction', 'congress', 'voted', 'month', 'huge', 'expansion', 'national', 'debt', 'pay', 'tax', '\\x1b[31mcut\\x1b[0m', 'called', 'fiscal', 'conservative', 'sunday', 'urged', 'budget', 'restraint', 'keeping', 'sharp']\n",
      "\u001b[35mRemoved stopwords (sample):\u001b[0m ['the', 'the', 'who', 'this', 'for', 'the', 'for', 'himself', 'and', 'with', 'under', 'among', 'the', 'which', 'are', 'over', 'when', 'they', 'from', 'the', 'will', 'other', 'such', 'even', 'the']\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "from itertools import zip_longest\n",
    "def _highlight_changes(before, after, n=25):\n",
    "    \"\"\"\n",
    "    Show first n items, marking changes in red.\n",
    "    Uses zip_longest so we can spot removals/insertions.\n",
    "    \"\"\"\n",
    "    shown = []\n",
    "    for b, a in zip_longest(before[:n], after[:n], fillvalue=None):\n",
    "        if b is None and a is not None:\n",
    "            shown.append(colored(a, 'red'))          \n",
    "        elif a is None and b is not None:\n",
    "            shown.append(colored(f\"{b}⟂\", 'red'))    \n",
    "        elif a != b:\n",
    "            shown.append(colored(a, 'red'))        \n",
    "        else:\n",
    "            shown.append(a)\n",
    "    return shown\n",
    "def visualize_pipeline(text, n=25):\n",
    "    text = \"\" if text is None else str(text)\n",
    "\n",
    "    tokens      = tokenize_text(text)\n",
    "    no_stop     = remove_stopwords(tokens)\n",
    "    stemmed     = stem_tokens(no_stop)\n",
    "    lemmatized  = lemmatize_tokens(no_stop) \n",
    "\n",
    "    print(colored(\"\\n--- VISUAL TOKEN TRANSFORMATIONS ---\", \"cyan\"))\n",
    "    print(colored(\"Original tokens:\", \"blue\"), tokens[:n])\n",
    "    print(colored(\"After stopword removal:\", \"green\"), no_stop[:n])\n",
    "\n",
    "    \n",
    "    print(colored(\"After stemming (changes in red):\", \"yellow\"),\n",
    "          _highlight_changes(no_stop, stemmed, n))\n",
    "    print(colored(\"After lemmatization (changes in red):\", \"yellow\"),\n",
    "          _highlight_changes(no_stop, lemmatized, n))\n",
    "\n",
    "\n",
    "    removed = [t for t in tokens if t not in no_stop]\n",
    "    if removed:\n",
    "        print(colored(\"Removed stopwords (sample):\", \"magenta\"), removed[:n])\n",
    "\n",
    "def visualize_pipeline_from_df(df, row=0, text_col=\"text\", n=25):\n",
    " \n",
    "    if text_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{text_col}' not found. Available: {list(df.columns)}\")\n",
    "    sample = df.iloc[row][text_col]\n",
    "    visualize_pipeline(sample, n=n)\n",
    "\n",
    "visualize_pipeline_from_df(train, row=0, text_col=\"text\", n=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ce5bd",
   "metadata": {},
   "source": [
    "2.4 Cleaning Pipe-lines: puts everything back into one cleaned string so that when we do any vectorized technique (CountVectorized, TfidfVectorizer), it shows as continious string as input, not Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90964f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"tokenize -> remove stopwords -> stem -> join (no lemmatization).\"\"\"\n",
    "    #text = \"\" if text is None else str(text)          # safety\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = stem_tokens(tokens)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def text_preprocessing(df_in: pd.DataFrame, text_col: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\"Apply clean_text() to a dataframe column and return a new dataframe with 'text_clean'.\"\"\"\n",
    "    df_out = df_in.copy()\n",
    "    if text_col not in df_out.columns:\n",
    "        raise KeyError(f\"Column '{text_col}' not found in the dataframe. Available: {list(df_out.columns)}\")\n",
    "    df_out[\"text_clean\"] = df_out[text_col].fillna(\"\").apply(clean_text)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc749979",
   "metadata": {},
   "source": [
    "## **3. Applying Processing on both training/test split from data.csv and on validation data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df08b0",
   "metadata": {},
   "source": [
    "3.1 Splitting the training data into train/test (80/20). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "272fc92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = train_test_split(\n",
    "    train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train[\"label\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d742d9",
   "metadata": {},
   "source": [
    "Note: I have 3 subset of data that all need to be cleaned before the model can understand them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93c750",
   "metadata": {},
   "source": [
    "3.2 Applying all the text-cleaning techniques built into every subset of the data. Without doing this then my training and validation data will be \"speaking different languages\" and my model will fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ec9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc = text_preprocessing(train_split, text_col=\"text\")\n",
    "test_proc = text_preprocessing(test_split, text_col=\"text\")\n",
    "val_proc = text_preprocessing(val, text_col=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a3ec6",
   "metadata": {},
   "source": [
    "3.3 This point was added, after checking that we had:\n",
    "- 552 empty rows in train\n",
    "- 144 empty rows in test\n",
    "- 23 empty rows in val\n",
    "And later for vectorizing is best to have 0 empty rows to avoid surprises later in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf53ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc.reset_index(drop=True, inplace=True)\n",
    "test_proc.reset_index(drop=True, inplace=True)\n",
    "val_proc.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07bcaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty rows in train: 552\n",
      "Empty rows in test : 144\n",
      "Empty rows in val  : 23 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Empty rows in train:\", (train_proc[\"text_clean\"].fillna(\"\").str.strip() == \"\").sum())\n",
    "print(\"Empty rows in test :\", (test_proc[\"text_clean\"].fillna(\"\").str.strip() == \"\").sum())\n",
    "print(\"Empty rows in val  :\", (val_proc[\"text_clean\"].fillna(\"\").str.strip() == \"\").sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b938fd",
   "metadata": {},
   "source": [
    "3.4 In this case, we are simplifying the DataFrame modeling. When I load the CSV, it has lots of columns. By applying the code below, we have the only 2 columns we need, which are \"label - target variable\" and \"text_clean - processed input feature\". \n",
    "This way we avoid the model to be fed with irrelevant fields and additionally to take less memory from our PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f6c4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep = [\"label\", \"text_clean\"]\n",
    "train_ready = train_proc[cols_keep].copy()\n",
    "test_ready  = test_proc[cols_keep].copy()\n",
    "val_ready   = val_proc[cols_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a22c8f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (31953, 2)\n",
      "   label                                         text_clean\n",
      "0      0                                  crook lie hillari\n",
      "1      0  justic scalia appear good health prior vacat c...\n",
      "2      0  univers north texa student critic condit nra r...\n",
      "3      0                                                   \n",
      "4      1  washington reuter robert mueller special couns... \n",
      "\n",
      "Test: (7989, 2)\n",
      "   label                                         text_clean\n",
      "0      1  washington reuter senat major leader mitch mcc...\n",
      "1      1  washington reuter state depart friday name rus...\n",
      "2      0  report expos email share wikileak show result ...\n",
      "3      1  washington reuter senat republican leader mitc...\n",
      "4      0  democrat stood american peopl republican shout... \n",
      "\n",
      "Val: (4956, 2)\n",
      "   label                                         text_clean\n",
      "0      2  london reuter british prime minist theresa reg...\n",
      "1      2  london reuter british counter terror polic mon...\n",
      "2      2  wellington reuter south pacif island nation sc...\n",
      "3      2  aden yemen reuter suspect qaeda milit kill dro...\n",
      "4      2  beij reuter chines academ publicli broach idea...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", train_ready.shape)\n",
    "print(train_ready.head(), \"\\n\")\n",
    "print(\"Test:\", test_ready.shape)\n",
    "print(test_ready.head(), \"\\n\")\n",
    "print(\"Val:\", val_ready.shape)\n",
    "print(val_ready.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef219320",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Train: 31,953 rows\n",
    "\n",
    "- Test: 7,989 rows\n",
    "\n",
    "- Validation: 4,956 rows\n",
    "\n",
    "text_clean is indeed cleaned\n",
    "From your sample rows:\n",
    "\n",
    "- Lowercased text ✅\n",
    "\n",
    "- Punctuation removed ✅\n",
    "\n",
    "- Stopwords removed ✅\n",
    "\n",
    "- Words appear stemmed (receiv, investig, violenc, defens, presid) ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36275b",
   "metadata": {},
   "source": [
    "3.5.2 Checking for label distribution. This information help us to make sure my split between train, test and validation are compatible and balanced before starting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a2a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels — train: [0, 1]\n",
      "Unique labels — test : [0, 1]\n",
      "Unique labels — val  : [2] \n",
      "\n",
      "Label distribution (counts)\n",
      "Train:\n",
      " label\n",
      "1    15999\n",
      "0    15954\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Test:\n",
      " label\n",
      "1    4000\n",
      "0    3989\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Val:\n",
      " label\n",
      "2    4956\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Label distribution (percent)\n",
      "Train:\n",
      " label\n",
      "1    50.07\n",
      "0    49.93\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "Test:\n",
      " label\n",
      "1    50.07\n",
      "0    49.93\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "Val:\n",
      " label\n",
      "2    100.0\n",
      "Name: proportion, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels — train:\", sorted(train_proc[\"label\"].unique()))\n",
    "print(\"Unique labels — test :\", sorted(test_proc[\"label\"].unique()))\n",
    "print(\"Unique labels — val  :\", sorted(val_proc[\"label\"].unique()), \"\\n\")\n",
    "\n",
    "print(\"Label distribution (counts)\")\n",
    "print(\"Train:\\n\", train_proc[\"label\"].value_counts(), \"\\n\")\n",
    "print(\"Test:\\n\",  test_proc[\"label\"].value_counts(),  \"\\n\")\n",
    "print(\"Val:\\n\",   val_proc[\"label\"].value_counts(),   \"\\n\")\n",
    "\n",
    "print(\"Label distribution (percent)\")\n",
    "print(\"Train:\\n\", (train_proc[\"label\"].value_counts(normalize=True) * 100).round(2), \"\\n\")\n",
    "print(\"Test:\\n\",  (test_proc[\"label\"].value_counts(normalize=True) * 100).round(2),  \"\\n\")\n",
    "print(\"Val:\\n\",   (val_proc[\"label\"].value_counts(normalize=True) * 100).round(2),   \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e7ae4",
   "metadata": {},
   "source": [
    "3.5 Saving the processed dataset after cleaning. This way we can reload the clean CSV later and go straight to vectorization and modeling without repeating the cleaning steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9844bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ready.to_csv(\"train_ready.csv\", index=False)\n",
    "test_ready.to_csv(\"test_ready.csv\", index=False)\n",
    "val_ready.to_csv(\"validation_ready.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IronHack1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
